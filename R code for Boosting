# Boosting

house=read.csv("/Users/song/Desktop/stat444/project/housing_price.csv")

##Data Cleanup

#Fill all empty cells with NA
house[house=='']<-NA

#bad data in YR_RMDL
house$YR_RMDL[house$YR_RMDL==20]<-2008

#bad data in STORIES
house$STORIES[house$STORIES==826]<-2
house$STORIES[house$STORIES==275]<-2
house$STORIES[house$STORIES==250]<-2
house$STORIES[house$STORIES==25]<-2
house$STORIES[house$STORIES==20]<-2

#bad data in LANDAREA
house$LANDAREA[house$LANDAREA==0]<-2294

#NA's in AYB, replace by first quantile
house$AYB[is.na(house$AYB)]<-1917

#NA's in YR_RMDL, replace by 0
house$YR_RMDL[is.na(house$YR_RMDL)]<-0

#NA's in STORIES, replace by median 2
house$STORIES[is.na(house$STORIES)]<-2

#NA's in KITCHENS, replace by median 1
house$KITCHENS[is.na(house$KITCHENS)]<-1

#Replace 'No Data' in HEAT by 'Forced Air'
house$HEAT[house$HEAT=='No Data']<-'Forced Air'

#Replace 0s in AC by 'Y'
house$AC[house$AC==0]<-'Y'

#Replace NA's in QUANZDRANT by 'NW'
house$QUADRANT[is.na(house$QUADRANT)]<-'NW'

#Replace NA's in ASSESSMENT_NBHD by 'Old City 1'
house$ASSESSMENT_NBHD[is.na(house$ASSESSMENT_NBHD)]<-'Old City 1'

#Replace NA's in Assessment_subnbhd
for (i in 1:39520){
  if (is.na(house$ASSESSMENT_SUBNBHD[i])) {
    house$ASSESSMENT_SUBNBHD2[i]=as.character(house$ASSESSMENT_NBHD[i])}
  else {house$ASSESSMENT_SUBNBHD2[i]=as.character(house$ASSESSMENT_SUBNBHD[i])}}
house$ASSESSMENT_SUBNBHD2<-as.factor(house$ASSESSMENT_SUBNBHD2)

#Replace NA's in CENSUS_BLOCK by '009000 1001'
house$CENSUS_BLOCK[is.na(house$CENSUS_BLOCK)]<-'009000 1001'

#Add new variable, log transformation of PRICE, GBA, LANDAREA, numeric transformation of SALEDATE
house$logprice<-log(house$PRICE)
house$logGBA<-log(house$GBA)
house$logLANDAREA<-log(house$LANDAREA)
house$numSALEDATE<-as.numeric(as.Date(house$SALEDATE))

#Adjust AC, GRADE, CNDTN to ordinal variables by assigning increasing levels
house$ordAC<-0
house$ordAC[house$AC=="N"]<-1
house$ordAC[house$AC=="Y"]<-2

house$ordGRADE<-0
house$ordGRADE[house$GRADE=="Low Quality"]<-1
house$ordGRADE[house$GRADE=="Fair Quality"]<-2
house$ordGRADE[house$GRADE=="Average"]<-3
house$ordGRADE[house$GRADE=="Above Average"]<-4
house$ordGRADE[house$GRADE=="Good Quality"]<-5
house$ordGRADE[house$GRADE=="Very Good"]<-6
house$ordGRADE[house$GRADE=="Excellent"]<-7
house$ordGRADE[house$GRADE=="Superior"]<-8
house$ordGRADE[house$GRADE=="Exceptional-A"]<-9
house$ordGRADE[house$GRADE=="Exceptional-B"]<-10
house$ordGRADE[house$GRADE=="Exceptional-C"]<-11
house$ordGRADE[house$GRADE=="Exceptional-D"]<-12

house$ordCNDTN<-0
house$ordCNDTN[house$CNDTN=="Poor"]<-1
house$ordCNDTN[house$CNDTN=="Fair"]<-2
house$ordCNDTN[house$CNDTN=="Average"]<-3
house$ordCNDTN[house$CNDTN=="Good"]<-4
house$ordCNDTN[house$CNDTN=="Very Good"]<-5
house$ordCNDTN[house$CNDTN=="Excellent"]<-6

#Error metric
rmsle<-function(y,pred){
  return(sqrt(1/length(y)*sum((y-pred)^2)))
}

#Helper Functions
get.explanatory_varnames <- function(formula){ f <- as.formula(formula)
terms <- terms(f)
attr(terms, "term.labels")
}

get.response <- function(fittedTree, test.data){
  f <- formula(fittedTree)
  terms <- terms(f)
  response.id <- attr(terms, "response")
  response <- as.list(attr(terms, "variables"))[[response.id + 1]]
  with(test.data, eval(response))}

# Strat Tuning
library(xgboost)

# Initialize formula
# Model 1: All variates
formula <- paste0("log(PRICE) ~ ",
                  paste("BATHRM", "HF_BATHRM", "HEAT", "ROOMS", "BEDRM", "ordAC", 
                        "AYB", "EYB", "STORIES", "logGBA", "STYLE", "QUADRANT",
                        "FIREPLACES", "logLANDAREA", "YR_RMDL", "ROOF", "KITCHENS",
                        "CENSUS_BLOCK", "WARD","EXTWALL","INTWALL", "USECODE",
                        "ZIPCODE", "LATITUDE", "LONGITUDE", "ASSESSMENT_NBHD",
                        "ASSESSMENT_SUBNBHD2", "CENSUS_TRACT", "numSALEDATE", "ordGRADE",
                        "ordCNDTN", "STRUCT",sep="+"))

# eta Selection
set.seed(67136)

# Fixed max.depth and nrounds 
max.depth <- 8
nrounds <- 300

# eta parameter values
etas <- c(0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1)
n_etas <- length(etas)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.eta <- cbind(as.data.frame(etas),rmsle=0)

for (i in 1:n_etas) {
  # Create mtry_rg_pred to store all precitions
  eta_xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    eta_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                           max.depth = max.depth, eta = etas[i], nrounds = nrounds
    )
    eta_xgboost_pred<-predict(eta_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),eta_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.eta[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(etas, rmsle_record.eta$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "eta", ylab="cv.error")

i <- which.min(rmsle_record.eta$rmsle)
eta <- etas[i] 

# max.depth Selection
set.seed(67136)

# Fixed nrounds 
nrounds <- 300

# max.depth parameter values
max.depths <- c(1, 3, 5, 7, 9, 11, 13)
n_max.depths <- length(max.depths)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.max.depth <- cbind(as.data.frame(max.depths),rmsle=0)

for (i in 1:n_max.depths) {
  # Create mtry_rg_pred to store all precitions
  max.depth_xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    max.depth_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                           max.depth = max.depths[i], eta = eta, nrounds = nrounds
    )
    max.depth_xgboost_pred<-predict(max.depth_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),max.depth_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.max.depth[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(max.depths, rmsle_record.max.depth$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "max.depth", ylab="cv.error")

i <- which.min(rmsle_record.max.depth$rmsle)
max.depth <- max.depths[i] 

# nrounds Selection
set.seed(67136)

# nrounds parameter values
nroundss <- c(100, 300, 500, 700, 900, 1100, 1300)
n_nroundss <- length(nroundss)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.nrounds <- cbind(as.data.frame(nroundss),rmsle=0)

for (i in 1:n_nroundss) {
  # Create mtry_rg_pred to store all precitions
  nrounds.xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    nrounds_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                                 max.depth = max.depth, eta = eta, nrounds = nroundss[i]
    )
    nrounds_xgboost_pred<-predict(nrounds_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),nrounds_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.nrounds[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(nroundss, rmsle_record.nrounds$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "nround", ylab="cv.error")

i <- which.min(rmsle_record.nrounds$rmsle)
nrounds <- nroundss[i] 

# Fit final model

boost_pred<-list()
for (j in 1:5){
  id.train <- house[house$fold!=j,]$Id
  id.test <- house[house$fold==j,]$Id
  test<-house[house$fold==j,]
  train<-house[house$fold!=j,]
  train2 <- train[, c("logprice",
                      get.explanatory_varnames(formula)) ]
  test2 <- test[, c("logprice",
                    get.explanatory_varnames(formula)) ]
  dtrain<-data.matrix(subset(train2,select=-c(logprice)))
  dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                     label = test2$logprice)
  xgboost <- xgboost(data=dtrain, label=train2$logprice,
                      max.depth = max.depth, eta = eta, nrounds = nrounds
  )
  Temp<-as.data.frame(predict(xgboost,dtest))
  Temp[,2]<-id.test
  # Initialize first step
  if (j==1) {xgboost_pred<-merge(boost_pred,Temp,all=TRUE)}
  # Update mtry_rg_pred
  xgboost_pred<-merge(xgboost_pred,Temp,all=TRUE)
}
xgboost_pred<-xgboost_pred[order(as.numeric(xgboost_pred$V2)),]
names(xgboost_pred)[1]<-'Prediction'
# Replicate the formula of global RMSLE with fold RMSLE
xgboost_pred<-xgboost_pred$Prediction
rmsle_record.final.1<-rmsle(house$logprice,xgboost_pred)

# Predictions and RMSLE of Model 1
xgboost_pred.1<-xgboost_pred
rmsle_record.final.1

#####

# Model 2: Remove variables with small importance according to RF cross-validation
formula <- paste0("log(PRICE) ~ ",
                  paste("BATHRM", "ROOMS", "BEDRM", "ordAC", "AYB", "EYB", "logGBA",
                        "FIREPLACES", "logLANDAREA", "YR_RMDL", "LATITUDE", "LONGITUDE",
                        "ZIPCODE", "CENSUS_TRACT", "numSALEDATE", "ordGRADE",
                        "ordCNDTN","CENSUS_BLOCK", "ASSESSMENT_NBHD", "ASSESSMENT_SUBNBHD2",
                        "WARD", "STORIES", sep="+"))

# eta Selection
set.seed(67136)

# Fixed max.depth and nrounds 
max.depth <- 8
nrounds <- 300

# eta parameter values
etas <- c(0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1)
n_etas <- length(etas)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.eta <- cbind(as.data.frame(etas),rmsle=0)

for (i in 1:n_etas) {
  # Create mtry_rg_pred to store all precitions
  eta_xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    eta_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                           max.depth = max.depth, eta = etas[i], nrounds = nrounds
    )
    eta_xgboost_pred<-predict(eta_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),eta_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.eta[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(etas, rmsle_record.eta$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "eta", ylab="cv.error")

i <- which.min(rmsle_record.eta$rmsle)
eta <- etas[i] 

# max.depth Selection
set.seed(67136)

# Fixed nrounds 
nrounds <- 300

# max.depth parameter values
max.depths <- c(1, 3, 5, 7, 9, 11, 13)
n_max.depths <- length(max.depths)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.max.depth <- cbind(as.data.frame(max.depths),rmsle=0)

for (i in 1:n_max.depths) {
  # Create mtry_rg_pred to store all precitions
  max.depth_xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    max.depth_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                                 max.depth = max.depths[i], eta = eta, nrounds = nrounds
    )
    max.depth_xgboost_pred<-predict(max.depth_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),max.depth_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.max.depth[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(max.depths, rmsle_record.max.depth$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "max.depth", ylab="cv.error")

i <- which.min(rmsle_record.max.depth$rmsle)
max.depth <- max.depths[i] 

# nrounds Selection
set.seed(67136)

# nrounds parameter values
nroundss <- c(100, 300, 500, 700, 900, 1100, 1300)
n_nroundss <- length(nroundss)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.nrounds <- cbind(as.data.frame(nroundss),rmsle=0)

for (i in 1:n_nroundss) {
  # Create mtry_rg_pred to store all precitions
  nrounds.xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    nrounds_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                               max.depth = max.depth, eta = eta, nrounds = nroundss[i]
    )
    nrounds_xgboost_pred<-predict(nrounds_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),nrounds_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.nrounds[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(nroundss, rmsle_record.nrounds$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "nround", ylab="cv.error")

i <- which.min(rmsle_record.nrounds$rmsle)
nrounds <- nroundss[i] 

# Fit final model

boost_pred<-list()
for (j in 1:5){
  id.train <- house[house$fold!=j,]$Id
  id.test <- house[house$fold==j,]$Id
  test<-house[house$fold==j,]
  train<-house[house$fold!=j,]
  train2 <- train[, c("logprice",
                      get.explanatory_varnames(formula)) ]
  test2 <- test[, c("logprice",
                    get.explanatory_varnames(formula)) ]
  dtrain<-data.matrix(subset(train2,select=-c(logprice)))
  dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                     label = test2$logprice)
  xgboost <- xgboost(data=dtrain, label=train2$logprice,
                     max.depth = max.depth, eta = eta, nrounds = nrounds
  )
  Temp<-as.data.frame(predict(xgboost,dtest))
  Temp[,2]<-id.test
  # Initialize first step
  if (j==1) {xgboost_pred<-merge(boost_pred,Temp,all=TRUE)}
  # Update mtry_rg_pred
  xgboost_pred<-merge(xgboost_pred,Temp,all=TRUE)
}
xgboost_pred<-xgboost_pred[order(as.numeric(xgboost_pred$V2)),]
names(xgboost_pred)[1]<-'Prediction'
# Replicate the formula of global RMSLE with fold RMSLE
xgboost_pred<-xgboost_pred$Prediction
rmsle_record.final.2<-rmsle(house$logprice,xgboost_pred)

# Predictions and RMSLE of Model 2
xgboost_pred.2<-xgboost_pred
rmsle_record.final.2

#####

# Model 3: Only numerical variables
formula <- paste0("log(PRICE) ~ ",
                  paste("BATHRM", "ROOMS", "BEDRM", "ordAC", "AYB", "EYB", "logGBA",
                        "FIREPLACES", "logLANDAREA", "YR_RMDL", "LATITUDE", "LONGITUDE",
                        "ZIPCODE", "CENSUS_TRACT", "numSALEDATE", "ordGRADE",
                        "ordCNDTN", sep="+"))

# eta Selection
set.seed(67136)

# Fixed max.depth and nrounds 
max.depth <- 8
nrounds <- 300

# eta parameter values
etas <- c(0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1)
n_etas <- length(etas)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.eta <- cbind(as.data.frame(etas),rmsle=0)

for (i in 1:n_etas) {
  # Create mtry_rg_pred to store all precitions
  eta_xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    eta_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                           max.depth = max.depth, eta = etas[i], nrounds = nrounds
    )
    eta_xgboost_pred<-predict(eta_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),eta_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.eta[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(etas, rmsle_record.eta$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "eta", ylab="cv.error")

i <- which.min(rmsle_record.eta$rmsle)
eta <- etas[i] 

# max.depth Selection
set.seed(67136)

# Fixed nrounds 
nrounds <- 300

# max.depth parameter values
max.depths <- c(1, 3, 5, 7, 9, 11, 13)
n_max.depths <- length(max.depths)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.max.depth <- cbind(as.data.frame(max.depths),rmsle=0)

for (i in 1:n_max.depths) {
  # Create mtry_rg_pred to store all precitions
  max.depth_xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    max.depth_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                                 max.depth = max.depths[i], eta = eta, nrounds = nrounds
    )
    max.depth_xgboost_pred<-predict(max.depth_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),max.depth_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.max.depth[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(max.depths, rmsle_record.max.depth$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "max.depth", ylab="cv.error")

i <- which.min(rmsle_record.max.depth$rmsle)
max.depth <- max.depths[i] 

# nrounds Selection
set.seed(67136)

# nrounds parameter values
nroundss <- c(100, 300, 500, 700, 900, 1100, 1300)
n_nroundss <- length(nroundss)
rmsle_fold_record <- numeric(length = 5)
rmsle_record.nrounds <- cbind(as.data.frame(nroundss),rmsle=0)

for (i in 1:n_nroundss) {
  # Create mtry_rg_pred to store all precitions
  nrounds.xgboost_pred<-list()
  for (j in 1:5){
    id.train <- house[house$fold!=j,]$Id
    id.test <- house[house$fold==j,]$Id
    test<-house[house$fold==j,]
    train<-house[house$fold!=j,]
    train2 <- train[, c("logprice",
                        get.explanatory_varnames(formula)) ]
    test2 <- test[, c("logprice",
                      get.explanatory_varnames(formula)) ]
    dtrain<-data.matrix(subset(train2,select=-c(logprice)))
    dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                       label = test2$logprice)
    nrounds_xgboost <- xgboost(data=dtrain, label=train2$logprice,
                               max.depth = max.depth, eta = eta, nrounds = nroundss[i]
    )
    nrounds_xgboost_pred<-predict(nrounds_xgboost,dtest)
    rmsle_fold_record[j]<-rmsle(get.response(formula, test),nrounds_xgboost_pred)
  }
  # Replicate the formula of global RMSLE with fold RMSLE
  rmsle_record.nrounds[i,2] <- sqrt(sum(rmsle_fold_record^2*7904)/39520)}

plot(nroundss, rmsle_record.nrounds$rmsle, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "nround", ylab="cv.error")

i <- which.min(rmsle_record.nrounds$rmsle)
nrounds <- nroundss[i] 

# Fit final model

boost_pred<-list()
for (j in 1:5){
  id.train <- house[house$fold!=j,]$Id
  id.test <- house[house$fold==j,]$Id
  test<-house[house$fold==j,]
  train<-house[house$fold!=j,]
  train2 <- train[, c("logprice",
                      get.explanatory_varnames(formula)) ]
  test2 <- test[, c("logprice",
                    get.explanatory_varnames(formula)) ]
  dtrain<-data.matrix(subset(train2,select=-c(logprice)))
  dtest<-xgb.DMatrix(data = data.matrix(subset(test2,select=-c(logprice))),
                     label = test2$logprice)
  xgboost <- xgboost(data=dtrain, label=train2$logprice,
                     max.depth = max.depth, eta = eta, nrounds = nrounds
  )
  Temp<-as.data.frame(predict(xgboost,dtest))
  Temp[,2]<-id.test
  # Initialize first step
  if (j==1) {xgboost_pred<-merge(boost_pred,Temp,all=TRUE)}
  # Update mtry_rg_pred
  xgboost_pred<-merge(xgboost_pred,Temp,all=TRUE)
}
xgboost_pred<-xgboost_pred[order(as.numeric(xgboost_pred$V2)),]
names(xgboost_pred)[1]<-'Prediction'
# Replicate the formula of global RMSLE with fold RMSLE
xgboost_pred<-xgboost_pred$Prediction
rmsle_record.final.3<-rmsle(house$logprice,xgboost_pred)

# Predictions and RMSLE of Model 3
xgboost_pred.3<-xgboost_pred
rmsle_record.final.3
